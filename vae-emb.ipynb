{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bbff990-3219-4a25-aeba-1c9c5d1adbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/jupyter-envs/generate-cross-species/atar-jupyter/conda/lib/python3.11/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "import torch\n",
    "import scipy as sp\n",
    "import json\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import anndata\n",
    "import pandas as pd\n",
    "from src.single_vae import VAE\n",
    "from src.multi_vae import CrossSpeciesVAE\n",
    "from src.callbacks import StageAwareEarlyStopping\n",
    "from src.data import CrossSpeciesDataModule\n",
    "import pickle\n",
    "from sklearn.metrics import adjusted_mutual_info_score, adjusted_rand_score\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "import hnswlib\n",
    "\n",
    "def _tanh_scale(x,scale=10,center=0.5):\n",
    "    return center + (1-center) * np.tanh(scale * (x - center))\n",
    "\n",
    "def _united_proj(wpca1, wpca2, k=20, metric=\"cosine\", ef=200, M=48):\n",
    "\n",
    "    metric = 'l2' if metric == 'euclidean' else metric\n",
    "    metric = 'cosine' if metric == 'correlation' else metric\n",
    "    labels2 = np.arange(wpca2.shape[0])\n",
    "    p2 = hnswlib.Index(space=metric, dim=wpca2.shape[1])\n",
    "    p2.init_index(max_elements=wpca2.shape[0], ef_construction=ef, M=M)\n",
    "    p2.add_items(wpca2, labels2)\n",
    "    p2.set_ef(ef)\n",
    "    idx1, dist1 = p2.knn_query(wpca1, k=k)\n",
    "\n",
    "    if metric == 'cosine':\n",
    "        dist1 = 1 - dist1\n",
    "        dist1[dist1 < 1e-3] = 1e-3\n",
    "        dist1 = dist1/dist1.max(1)[:,None]\n",
    "        dist1 = _tanh_scale(dist1,scale=10, center=0.7)\n",
    "    else:\n",
    "        sigma1 = dist1[:,4]\n",
    "        sigma1[sigma1<1e-3]=1e-3\n",
    "        dist1 = np.exp(-dist1/sigma1[:,None])\n",
    "        \n",
    "    Sim1 = dist1  # np.exp(-1*(1-dist1)**2)\n",
    "    knn1v2 = sp.sparse.lil_matrix((wpca1.shape[0], wpca2.shape[0]))\n",
    "    x1 = np.tile(np.arange(idx1.shape[0])[:, None], (1, idx1.shape[1])).flatten()\n",
    "    knn1v2[x1.astype('int32'), idx1.flatten().astype('int32')] = Sim1.flatten()\n",
    "    return knn1v2.tocsr()\n",
    "\n",
    "from pynndescent import NNDescent\n",
    "\n",
    "def find_nearest_neighbors(L1, L2, n_neighbors=15, metric='correlation'):\n",
    "    \"\"\"\n",
    "    Finds the nearest neighbors from L1 (query) to L2 (index) using pynndescent.\n",
    "\n",
    "    Parameters:\n",
    "        L1 (np.ndarray): Query embeddings of shape (num_queries, embedding_dim).\n",
    "        L2 (np.ndarray): Index embeddings of shape (num_index, embedding_dim).\n",
    "        n_neighbors (int): Number of neighbors to find. Default is 5.\n",
    "        metric (str): Distance metric to use. Default is 'euclidean'.\n",
    "\n",
    "    Returns:\n",
    "        indices (np.ndarray): Indices of nearest neighbors in L2 for each query in L1.\n",
    "        distances (np.ndarray): Distances to nearest neighbors for each query in L1.\n",
    "    \"\"\"\n",
    "    # Validate inputs\n",
    "    if not isinstance(L1, np.ndarray) or not isinstance(L2, np.ndarray):\n",
    "        raise ValueError(\"L1 and L2 must be numpy arrays.\")\n",
    "    \n",
    "    if L1.shape[1] != L2.shape[1]:\n",
    "        raise ValueError(\"L1 and L2 must have the same embedding dimension.\")\n",
    "\n",
    "    # Build the index on L2\n",
    "    index = NNDescent(L2, metric=metric, n_neighbors=n_neighbors)\n",
    "    \n",
    "    # Query the nearest neighbors for L1\n",
    "    indices, distances = index.query(L1, k=n_neighbors)\n",
    "    \n",
    "    return indices, distances"
   ]
  },
  {
   "cell_type": "raw",
   "id": "46aabcb7-44d1-4fa6-81de-b7e1532181c0",
   "metadata": {},
   "source": [
    "adata1 = anndata.read_h5ad('data/fish/full.h5ad')\n",
    "adata2 = anndata.read_h5ad('data/frog/full.h5ad')\n",
    "\n",
    "adata1.X = adata1.X.astype('float32')\n",
    "adata2.X = adata2.X.astype('float32')\n",
    "\n",
    "emb1 = adata1.varm['esm']\n",
    "emb2 = adata2.varm['esm']\n",
    "\n",
    "emb1 = torch.from_numpy(emb1).float()\n",
    "emb2 = torch.from_numpy(emb2).float()\n",
    "\n",
    "XY_raw = _united_proj(emb1.numpy(), emb2.numpy(), k=25, metric='euclidean') # 25 IS THE BEST I'VE TRIED (I tried 10, 50, 100) - HOW CAN I DETERMINE THIS MORE PROGRAMMATICALLY?\n",
    "YX_raw = _united_proj(emb2.numpy(), emb1.numpy(), k=25, metric='euclidean')\n",
    "\n",
    "XY = XY_raw.copy()\n",
    "YX = YX_raw.copy()\n",
    "XY.data[:]=1\n",
    "YX.data[:]=1\n",
    "\n",
    "G = XY + YX.T\n",
    "\n",
    "G.data[G.data>1]=0\n",
    "G.eliminate_zeros()\n",
    "x, y = G.nonzero()\n",
    "\n",
    "\n",
    "G = XY_raw/2 + YX_raw.T/2\n",
    "G[x,y] = 0\n",
    "G.eliminate_zeros()\n",
    "x, y = G.nonzero()\n",
    "\n",
    "homology_edges = {}\n",
    "homology_edges[0] = {}\n",
    "homology_edges[0][1] = torch.tensor(np.vstack((x,y)).T)\n",
    "\n",
    "homology_edges[1] = {}\n",
    "homology_edges[1][0] = torch.tensor(np.vstack((y,x)).T)\n",
    "\n",
    "homology_scores = {}\n",
    "homology_scores[0] = {}\n",
    "homology_scores[0][1] = torch.tensor(G.data).float()\n",
    "\n",
    "homology_scores[1] = {}\n",
    "homology_scores[1][0] = torch.tensor(G.data).float()\n",
    "\n",
    "pickle.dump((homology_edges,homology_scores), open('homology_zfxe.p','wb'))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "23006ee5-d69e-4610-bbd8-cf9b01002ca6",
   "metadata": {},
   "source": [
    "# adata1 = adata1[adata1.obs['TimeID'] != '4hpf'].copy()\n",
    "# adata2 = adata2[adata2.obs['Developmental_stage'] != 'Stage_8'].copy()\n",
    "\n",
    "species_data = {\n",
    "    \"fish\": adata1,\n",
    "    \"frog\": adata2,\n",
    "}\n",
    "\n",
    "emb_data = {\n",
    "    \"fish\": emb1,\n",
    "    \"frog\": emb2, \n",
    "}\n",
    "data_module = CrossSpeciesDataModule(\n",
    "    species_data = species_data,\n",
    "    batch_size=512,\n",
    "    num_workers=0,\n",
    "    val_split=0.001,\n",
    "    test_split=0.001,\n",
    "    yield_pairwise=False,\n",
    "    subsample_size=10000,\n",
    "    subsample_by={\n",
    "        \"fish\": \"cell_type\",\n",
    "        \"frog\": \"cell_type\",   \n",
    "    }\n",
    ")\n",
    "data_module.setup()\n",
    "\n",
    "species_data_sub = {k: data_module.train_dataset.epoch_data[k][data_module.train_dataset.epoch_indices[k]].copy() for k in data_module.train_dataset.epoch_data}\n",
    "\n",
    "pickle.dump(species_data_sub, open('data.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fd549e2-a5dc-449c-b65d-c50cab373362",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "species_data_sub = pickle.load(open('data.p','rb'))\n",
    "\n",
    "adata1 = species_data_sub['fish']\n",
    "adata2 = species_data_sub['frog']\n",
    "\n",
    "gene_emb = {0: torch.tensor(adata1.varm['esm']), 1: torch.tensor(adata2.varm['esm'])}\n",
    "batch_size = 64\n",
    "\n",
    "data_module = CrossSpeciesDataModule(\n",
    "    species_data = species_data_sub,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=0,\n",
    "    val_split=0.1,\n",
    "    test_split=0.1,\n",
    "    yield_pairwise=False,\n",
    ")\n",
    "data_module.setup()\n",
    "\n",
    "species_vocab_sizes = data_module.species_vocab_sizes\n",
    "homology_edges, homology_scores = pickle.load(open('homology_zfxe.p','rb'))\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.001,\n",
    "    patience=3,\n",
    "    verbose=True,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "\n",
    "model = CrossSpeciesVAE(\n",
    "    species_vocab_sizes=species_vocab_sizes,\n",
    "    gene_embeddings=gene_emb,\n",
    "    batch_size=batch_size,\n",
    "    \n",
    "    # Loss weights\n",
    "    recon_weight=1.0,\n",
    "    aggregator_dim=256,\n",
    "    \n",
    "    # Testing\n",
    "    n_clusters=100,\n",
    "    cluster_warmup_epochs=3,\n",
    "    initial_alpha=1.0,\n",
    "    \n",
    "\n",
    "    # Learning rate\n",
    "    base_learning_rate=5e-3,\n",
    "    min_learning_rate=5e-5,    \n",
    "    warmup_data=0.1,\n",
    "\n",
    ")\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    max_epochs=20,\n",
    "    precision='16-mixed',\n",
    "    gradient_clip_val=model.gradient_clip_val,\n",
    "    gradient_clip_algorithm=\"norm\",\n",
    "    log_every_n_steps=1,\n",
    "    deterministic=True,\n",
    "    callbacks=[early_stopping],\n",
    "    accumulate_grad_batches=1,\n",
    "    enable_progress_bar=True,\n",
    "    fast_dev_run=False,\n",
    "    logger=CSVLogger(\n",
    "        save_dir=\"logs\",\n",
    "        name=\"metrics\",\n",
    "        flush_logs_every_n_steps=10\n",
    "    )    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab77d7b-8be9-44e7-81ac-5f0d05392cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/opt/jupyter-envs/generate-cross-species/atar-jupyter/conda/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n",
      "/opt/jupyter-envs/generate-cross-species/atar-jupyter/conda/lib/python3.11/site-packages/pytorch_lightning/utilities/data.py:123: Your `IterableDataset` has `__len__` defined. In combination with multi-process data loading (when num_workers > 1), `__len__` could be inaccurate if each worker is not configured independently to avoid having duplicate data.\n",
      "\n",
      "  | Name            | Type                | Params | Mode \n",
      "----------------------------------------------------------------\n",
      "0 | esm_aggregators | ModuleDict          | 13.4 M | train\n",
      "1 | encoder         | Encoder             | 197 K  | train\n",
      "2 | clusterer       | ParametricClusterer | 25.7 K | train\n",
      "3 | decoders        | ModuleDict          | 25.4 M | train\n",
      "  | other params    | n/a                 | 1      | n/a  \n",
      "----------------------------------------------------------------\n",
      "39.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "39.0 M    Total params\n",
      "155.839   Total estimated model params size (MB)\n",
      "83        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/jupyter-envs/generate-cross-species/atar-jupyter/conda/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 125/125 [00:25<00:00,  4.89it/s, v_num=436]      \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|▋         | 1/16 [00:00<00:00, 16.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|█▎        | 2/16 [00:00<00:01, 12.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|█▉        | 3/16 [00:00<00:01, 12.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|██▌       | 4/16 [00:00<00:01, 11.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|███▏      | 5/16 [00:00<00:00, 11.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|███▊      | 6/16 [00:00<00:00, 11.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|████▍     | 7/16 [00:00<00:00, 11.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 8/16 [00:00<00:00, 11.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████▋    | 9/16 [00:00<00:00, 11.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████▎   | 10/16 [00:00<00:00, 11.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|██████▉   | 11/16 [00:00<00:00, 11.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████▌  | 12/16 [00:01<00:00, 11.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████▏ | 13/16 [00:01<00:00, 11.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|████████▊ | 14/16 [00:01<00:00, 11.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|█████████▍| 15/16 [00:01<00:00, 11.08it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 16/16 [00:01<00:00, 11.07it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 125/125 [00:27<00:00,  4.62it/s, v_num=436]   \u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 125/125 [00:25<00:00,  4.89it/s, v_num=436]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|▋         | 1/16 [00:00<00:00, 16.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|█▎        | 2/16 [00:00<00:01, 12.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|█▉        | 3/16 [00:00<00:01, 12.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|██▌       | 4/16 [00:00<00:01, 11.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|███▏      | 5/16 [00:00<00:00, 11.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|███▊      | 6/16 [00:00<00:00, 11.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|████▍     | 7/16 [00:00<00:00, 11.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 8/16 [00:00<00:00, 11.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████▋    | 9/16 [00:00<00:00, 11.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████▎   | 10/16 [00:00<00:00, 11.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|██████▉   | 11/16 [00:00<00:00, 11.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████▌  | 12/16 [00:01<00:00, 11.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████▏ | 13/16 [00:01<00:00, 11.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|████████▊ | 14/16 [00:01<00:00, 11.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|█████████▍| 15/16 [00:01<00:00, 11.03it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 16/16 [00:01<00:00, 11.03it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 125/125 [00:27<00:00,  4.61it/s, v_num=436]   \u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.014 >= min_delta = 0.001. New best score: 0.385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  10%|█         | 13/125 [00:02<00:25,  4.35it/s, v_num=436] "
     ]
    }
   ],
   "source": [
    "trainer.fit(model, data_module)\n",
    "print(trainer.current_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d7c10b-898d-4cce-98c2-cc52b7a01882",
   "metadata": {},
   "outputs": [],
   "source": [
    ",# Load the latest version\n",
    "version = max([int(d.split('_')[-1]) for d in glob('logs/metrics/version_*')])\n",
    "df = pd.read_csv(f'logs/metrics/version_{version}/metrics.csv')\n",
    "\n",
    "\n",
    "for loss_type in [\"loss\", \"recon\", \"kl\", \"alpha\"]:\n",
    "    # Get specific metrics\n",
    "    train_loss = df[f'train_{loss_type}'][df[f'train_{loss_type}'] != 0.0]\n",
    "    train_step = df['step'][df[f'train_{loss_type}'] != 0.0]\n",
    "    f1 = ~train_loss.isna()\n",
    "    val_loss =  df[f'val_{loss_type}']#[df[f'val_{loss_type}'] != 0.0]\n",
    "    val_step = df['step']#[df[f'val_{loss_type}'] != 0.0]    \n",
    "    f2 = ~val_loss.isna()\n",
    "\n",
    "    # Plot\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.plot(train_step[f1], train_loss[f1], label='Training Loss')\n",
    "    plt.plot(val_step[f2], val_loss[f2], label='Validation Loss')\n",
    "\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(loss_type)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1de5e6a-cb71-4331-bb64-2d9784fac4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "output, s, memberships = model.get_latent_embeddings(species_data_sub, batch_size=32)\n",
    "latent = output.cpu().numpy()\n",
    "s=s.cpu().numpy()\n",
    "\n",
    "#output, s = model.get_latent_embeddings(species_data)\n",
    "#s=s.cpu().numpy()\n",
    "\n",
    "#plt.figure();\n",
    "#dt_full = umap.UMAP(n_components=2).fit_transform(output.detach().cpu().numpy())\n",
    "#plt.scatter(dt_full[:,0],dt_full[:,1], c = s,s=5, cmap='rainbow'); plt.colorbar()\n",
    "\n",
    "for i, adata, key in zip([0, 1], [species_data_sub['fish'], species_data_sub['frog']], ['cell_type', 'cell_type']):\n",
    "    # dt = dt_full[s==i] # umap.UMAP(n_components=2).fit_transform(output[s==i].detach().cpu().numpy())\n",
    "\n",
    "    adata.obsm['emb'] = latent[s==i]\n",
    "    sc.pp.neighbors(adata, use_rep='emb')\n",
    "    sc.tl.leiden(adata, flavor=\"igraph\", n_iterations=2, resolution=2)\n",
    "    print(f\"Species {i+1}\", adjusted_rand_score(adata.obs[key], adata.obs['leiden']), adjusted_mutual_info_score(adata.obs[key], adata.obs['leiden']))\n",
    "    \n",
    "#     plt.figure();\n",
    "#     plt.scatter(dt[:,0],dt[:,1], c = adata.obs[key].cat.codes,s=5, cmap='rainbow'); plt.colorbar()\n",
    "    \n",
    "adatas = []\n",
    "for i, k in enumerate(species_data_sub):\n",
    "    adata = species_data_sub[k]\n",
    "    adata.obsm['emb'] = latent[s==i]\n",
    "    adata.obs['species'] = k\n",
    "    adatas.append(adata)\n",
    "\n",
    "adata = anndata.concat(adatas,join='outer')\n",
    "\n",
    "nnm1v2 = _united_proj(latent[s==0],latent[s==1], k=15, metric='cosine')\n",
    "nnm2v1 = _united_proj(latent[s==1],latent[s==0], k=15, metric='cosine')\n",
    "\n",
    "sc.pp.neighbors(adata, use_rep='emb')\n",
    "adata.obsp['connectivities'] = sp.sparse.vstack((sp.sparse.hstack((sp.sparse.csr_matrix((nnm1v2.shape[0],nnm1v2.shape[0])), nnm1v2)),\n",
    "sp.sparse.hstack((nnm2v1, sp.sparse.csr_matrix((nnm2v1.shape[0],nnm2v1.shape[0]))))))\n",
    "sc.tl.leiden(adata, flavor=\"igraph\", n_iterations=2, resolution=2)\n",
    "print(\"Cross species\", adjusted_rand_score(adata.obs['cell_type'], adata.obs['leiden']))\n",
    "\n",
    "x,y = nnm1v2.nonzero()\n",
    "cl1 = np.array(list(adata1.obs['cell_type']))\n",
    "cl2 = np.array(list(adata2.obs['cell_type']))\n",
    "\n",
    "a = cl2[y.reshape((nnm1v2.shape[0], 15))]\n",
    "cl1_new = []\n",
    "for i in range(a.shape[0]):\n",
    "    b,v = np.unique(a[i],return_counts=True)\n",
    "    cl1_new.append(b[np.argmax(v)])\n",
    "\n",
    "cl1_new = np.array(cl1_new)\n",
    "\n",
    "print(\"Cross species 1\", adjusted_rand_score(adata1.obs['cell_type'], cl1_new), adjusted_mutual_info_score(adata1.obs['cell_type'], cl1_new))\n",
    "print(\"Cross species 1 leiden\", adjusted_rand_score(adata1.obs['leiden'], cl1_new), adjusted_mutual_info_score(adata1.obs['leiden'], cl1_new))\n",
    "\n",
    "x,y = nnm2v1.nonzero()\n",
    "\n",
    "a = cl1[y.reshape((nnm2v1.shape[0], 15))]\n",
    "cl2_new = []\n",
    "for i in range(a.shape[0]):\n",
    "    b,v = np.unique(a[i],return_counts=True)\n",
    "    cl2_new.append(b[np.argmax(v)])\n",
    "\n",
    "cl2_new = np.array(cl2_new)\n",
    "\n",
    "print(\"Cross species 2\", adjusted_rand_score(adata2.obs['cell_type'], cl2_new), adjusted_mutual_info_score(adata2.obs['cell_type'], cl2_new))\n",
    "print(\"Cross species 2 leiden\", adjusted_rand_score(adata2.obs['leiden'], cl2_new), adjusted_mutual_info_score(adata2.obs['leiden'], cl2_new))\n",
    "\n",
    "plt.figure();\n",
    "dt_full = umap.UMAP(n_components=2).fit_transform(output.detach().cpu().numpy())\n",
    "plt.scatter(dt_full[:,0],dt_full[:,1],c=s,s=1, alpha=0.5, cmap='rainbow');"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0891e785-c696-4a35-8774-f8fd91fd9f79",
   "metadata": {},
   "source": [
    "c_memberships1 = memberships[0].detach().cpu().numpy()\n",
    "c1 = np.argmax(c_memberships1,axis=1)\n",
    "\n",
    "c_memberships2 = memberships[1].detach().cpu().numpy()\n",
    "c2 = np.argmax(c_memberships2,axis=1)\n",
    "\n",
    "c = c1\n",
    "\n",
    "print(adjusted_rand_score(c1,c2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e6653b-7c9d-49b9-adde-28db8f1be97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_memberships = memberships.detach().cpu().numpy()\n",
    "c = np.argmax(c_memberships,axis=1)\n",
    "c1=c2=c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab18c50-ba15-4ef4-ac6f-a4d6eb6940ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(); plt.scatter(dt_full[:,0],dt_full[:,1],c=c1,cmap='rainbow',s=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee770ea-577c-4611-9bb6-8844d268394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(); plt.scatter(dt_full[:,0],dt_full[:,1],c=c2,cmap='rainbow',s=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bce89f6-455f-4893-8a9d-44113cabee2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(); plt.scatter(dt_full[s==0,0],dt_full[s==0,1],c=c1[s==0],cmap='rainbow',s=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1675c5f9-538a-4ee2-87db-5f0ef2c216b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(); plt.scatter(dt_full[s==1,0],dt_full[s==1,1],c=c2[s==1],cmap='rainbow',s=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b830f480-75c2-47d0-b4b6-2d57755e2ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1894494-d32c-4421-8a54-3221fffefb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_mutual_info_score(species_data_sub['fish'].obs['cell_type'],c1[s==0]), adjusted_mutual_info_score(species_data_sub['fish'].obs['cell_type'],species_data_sub['fish'].obs['leiden'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4368aa-7ea4-4521-811e-e9745a30d6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_mutual_info_score(species_data_sub['frog'].obs['cell_type'],c1[s==1]), adjusted_mutual_info_score(species_data_sub['frog'].obs['cell_type'],species_data_sub['frog'].obs['leiden'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe44069d-bf74-4d71-8fbf-f2b7ec0c74ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_mutual_info_score(species_data_sub['fish'].obs['cell_type'],c2[s==0]), adjusted_mutual_info_score(species_data_sub['fish'].obs['cell_type'],species_data_sub['fish'].obs['leiden'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a16a0b-5504-4bf8-af4b-054921571588",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_mutual_info_score(species_data_sub['frog'].obs['cell_type'],c2[s==1]), adjusted_mutual_info_score(species_data_sub['frog'].obs['cell_type'],species_data_sub['frog'].obs['leiden'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d78cfde-5d2e-4231-a68b-7e381d2e0136",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.exp(model.clusterer.log_sigma).max(), torch.exp(model.clusterer.log_sigma).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85f2291-5fbe-4a46-83b1-9333ed108ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = np.unique(c2,return_counts=True)\n",
    "x[y>5].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437ac3ae-005b-4b93-9220-781b883e6a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = np.unique(c1,return_counts=True)\n",
    "x[y>5].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cbd2ad-7838-4730-a75f-e3f0e10d432d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "labels = KMeans(n_clusters=100).fit_predict(output.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480ad5ea-b211-4269-b35e-928e322e8752",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_mutual_info_score(species_data_sub['fish'].obs['cell_type'],labels[s==0]), adjusted_mutual_info_score(species_data_sub['fish'].obs['cell_type'],species_data_sub['fish'].obs['leiden'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c674b5-66cb-48b9-a333-304849bbabfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_mutual_info_score(species_data_sub['frog'].obs['cell_type'],labels[s==1]), adjusted_mutual_info_score(species_data_sub['frog'].obs['cell_type'],species_data_sub['frog'].obs['leiden'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2dd2e7-7e51-4396-afa0-9795a8207966",
   "metadata": {},
   "outputs": [],
   "source": [
    "score1 = silhouette_score(output.cpu().numpy()[s==0], c1[s==0])\n",
    "score2 = silhouette_score(output.cpu().numpy()[s==0], labels[s==0])\n",
    "print(\"Silhouette score model:\", score1)\n",
    "print(\"Silhouette score reference:\", score2)\n",
    "print(\"Ratio:\", score1/score2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f2746f-b14e-4f31-9c01-7bfef2a29549",
   "metadata": {},
   "outputs": [],
   "source": [
    "score1 = silhouette_score(output.cpu().numpy()[s==1], c2[s==1])\n",
    "score2 = silhouette_score(output.cpu().numpy()[s==1], labels[s==1])\n",
    "print(\"Silhouette score model:\", score1)\n",
    "print(\"Silhouette score reference:\", score2)\n",
    "print(\"Ratio:\", score1/score2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedfcc78-4606-4338-8bc3-bc18d679e59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "z=[]\n",
    "for i in np.unique(labels):\n",
    "    v = s[labels==i]\n",
    "    j = (v==0).mean()\n",
    "    z.append(max(j,1-j))\n",
    "    \n",
    "np.mean(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e99a7d-60e0-484a-8d9c-3404eb6c10f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "z=[]\n",
    "for i in np.unique(c):\n",
    "    v = s[c==i]\n",
    "    j = (v==0).mean()\n",
    "    z.append(max(j,1-j))\n",
    "    \n",
    "np.mean(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2370566-58f3-4e87-9aea-7a2b558b77b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = output.cpu().numpy()\n",
    "adatas = []\n",
    "for i, k in enumerate(species_data_sub):\n",
    "    adata = species_data_sub[k]\n",
    "    adata.obsm['emb'] = latent[s==i]\n",
    "    adata.obs['species'] = k\n",
    "    adatas.append(adata)\n",
    "\n",
    "adata = anndata.concat(adatas,join='outer')\n",
    "adata.obsm['X_umap'] = dt_full\n",
    "\n",
    "import scanpy as sc\n",
    "\n",
    "sc.external.pp.harmony_integrate(adata, \"species\", basis='emb', adjusted_basis='X_pca_harmony')\n",
    "\n",
    "adata.obsm['X_umap'] = umap.UMAP().fit_transform(adata.obsm['X_pca_harmony'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676c9ede-a2b4-449d-b1fd-354b884016c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(adata,color='species')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0b6207-eb02-4c8f-84e6-b3d211f17dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_label_similarity_matrix(latent, s, i, j, name1, name2, K):\n",
    "    nnm = _united_proj(latent[s==j],latent[s==i], k=K, metric='cosine')\n",
    "    x, y = nnm.nonzero()\n",
    "    indices_x = x.reshape((nnm.shape[0],K))\n",
    "    indices_y = y.reshape((nnm.shape[0],K))\n",
    "    similarities = nnm.data.reshape((nnm.shape[0],K))\n",
    "\n",
    "\n",
    "    cl1 = np.array(list(species_data_sub['fish'].obs[name1]))\n",
    "    cl2 = np.array(list(species_data_sub['frog'].obs[name2]))\n",
    "\n",
    "    clu1,cluc1 = np.unique(cl1, return_counts=True)\n",
    "    clu2,cluc2 = np.unique(cl2, return_counts=True)\n",
    "\n",
    "    C = np.zeros((clu1.size,clu2.size))\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame();\n",
    "    df['labels_0'] = cl1[indices_y].flatten()\n",
    "    df['labels_1'] = cl2[indices_x].flatten()\n",
    "    df['similarities'] = similarities.flatten()\n",
    "    df = df.groupby(['labels_0','labels_1']).sum().reset_index()\n",
    "\n",
    "    C[pd.Series(index=clu1,data=np.arange(clu1.size))[df['labels_0']].values, pd.Series(index=clu2,data=np.arange(clu2.size))[df['labels_1']].values] = df['similarities'].values\n",
    "    C = np.stack((C / cluc1[:,None], C/cluc2[None,:]),axis=2).min(2)\n",
    "    \n",
    "    return C, clu1, clu2\n",
    "\n",
    "\n",
    "def format_cross_species_similarities(\n",
    "    similarity_matrix: np.ndarray,\n",
    "    source_labels: np.ndarray,\n",
    "    target_labels: np.ndarray,\n",
    "    source_species: str = \"fish\",\n",
    "    target_species: str = \"frog\",\n",
    "    top_n: int | None = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Format similarity matrix into a DataFrame with columns for each species-cell type combination.\n",
    "    \n",
    "    Args:\n",
    "        similarity_matrix: NxM matrix of similarities\n",
    "        source_labels: N labels from source species\n",
    "        target_labels: M labels from target species\n",
    "        source_species: Name of source species\n",
    "        target_species: Name of target species\n",
    "        top_n: Optional number of top matches to include (None for all)\n",
    "    \"\"\"\n",
    "    # Create columns for both species\n",
    "    columns = pd.MultiIndex.from_tuples(\n",
    "        # Wagner columns\n",
    "        [(source_species, cell_type, col_name)\n",
    "         for cell_type in source_labels\n",
    "         for col_name in ['matched_type', 'similarity']] +\n",
    "        # Briggs columns\n",
    "        [(target_species, cell_type, col_name)\n",
    "         for cell_type in target_labels\n",
    "         for col_name in ['matched_type', 'similarity']]\n",
    "    )\n",
    "    \n",
    "    # Initialize DataFrame\n",
    "    max_rows = max(\n",
    "        len(target_labels) if top_n is None else top_n,\n",
    "        len(source_labels) if top_n is None else top_n\n",
    "    )\n",
    "    df = pd.DataFrame(index=range(max_rows), columns=columns)\n",
    "    \n",
    "    # Fill Wagner -> Briggs mappings\n",
    "    for i, source_label in enumerate(source_labels):\n",
    "        similarities = similarity_matrix[i]\n",
    "        sorted_indices = np.argsort(similarities)[::-1]\n",
    "        if top_n is not None:\n",
    "            sorted_indices = sorted_indices[:top_n]\n",
    "            \n",
    "        df[(source_species, source_label, 'matched_type')] = target_labels[sorted_indices]\n",
    "        df[(source_species, source_label, 'similarity')] = similarities[sorted_indices]\n",
    "    \n",
    "    # Fill Briggs -> Wagner mappings (using transposed similarity matrix)\n",
    "    similarity_matrix_T = similarity_matrix.T\n",
    "    for i, target_label in enumerate(target_labels):\n",
    "        similarities = similarity_matrix_T[i]\n",
    "        sorted_indices = np.argsort(similarities)[::-1]\n",
    "        if top_n is not None:\n",
    "            sorted_indices = sorted_indices[:top_n]\n",
    "            \n",
    "        df[(target_species, target_label, 'matched_type')] = source_labels[sorted_indices]\n",
    "        df[(target_species, target_label, 'similarity')] = similarities[sorted_indices]\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb92e3f-f1b7-48cd-8d70-a00f17cd483c",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = adata.obsm['X_pca_harmony']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22b482c-a02c-4e0b-b852-4a550cf23ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "species_data_sub['fish'].obs['cell_type_strip'] = pd.Categorical(['-'.join(i.split('-')[1:]) for i in species_data_sub['fish'].obs['cell_type']])\n",
    "species_data_sub['frog'].obs['cell_type_strip'] = pd.Categorical(['-'.join(i.split('-')[1:]) for i in species_data_sub['frog'].obs['cell_type']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05aedf65-7246-4351-9c1c-89967c9cfcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 50\n",
    "# latent = output.cpu().numpy()\n",
    "C, clu1, clu2 = generate_label_similarity_matrix(latent, s, 0, 1, 'cell_type_strip', 'cell_type_strip', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77761a70-d0b6-4fb2-84ef-ace3601521bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Create the DataFrame with top 5 matches\n",
    "similarity_df1 = format_cross_species_similarities(C, clu1, clu2, top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1896ca90-54d9-48c9-afb6-9d8b7120dbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_df1['frog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609ea634-96f8-45fe-b33f-ad716c2569e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_df1['fish']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f043c8d8-6bb7-48dc-a27c-1470518ad527",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2bc36f-e7be-46e0-98df-8929022f2636",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
